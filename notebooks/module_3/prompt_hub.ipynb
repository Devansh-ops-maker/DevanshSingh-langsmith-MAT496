{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connecting to the Prompt Hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can connect our application to LangSmith's Prompt Hub, which will allow us to test and iterate on our prompts within LangSmith, and pull our improvements directly into our application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pull a prompt from Prompt Hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pull in a prompt from Prompt Hub by pasting in the code snippet from the UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ayush singh\\appdata\\local\\programs\\python\\python39\\lib\\json\\decoder.py:337: UserWarning: WARNING! extra_headers is not default parameter.\n",
      "                extra_headers was transferred to model_kwargs.\n",
      "                Please confirm that extra_headers is what you intended.\n",
      "  obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langsmith import Client\n",
    "LANGCHAIN_API_KEY=os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "client = Client(api_key=LANGCHAIN_API_KEY)\n",
    "prompt = client.pull_prompt(\"pirate-friend\", include_model=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what we pulled - note that we did not get the model, so this is just a StructuredPrompt and not runnable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructuredPrompt(input_variables=['language', 'question'], input_types={}, partial_variables={}, metadata={'lc_hub_owner': '-', 'lc_hub_repo': 'pirate-friend', 'lc_hub_commit_hash': '826e06f1702c5777d78dc1cb06bd04ae8eb55f23a82b6504752d44a4cb95bd31'}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['language'], input_types={}, partial_variables={}, template='You are a pirate from 1600s, you only seak {language}'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='{question}'), additional_kwargs={})], schema_={'title': 'answer', 'description': 'Extract the answer', 'type': 'object', 'properties': {'answer': {'type': 'string', 'description': 'The answer from the LLM to the user '}}, 'required': ['answer'], 'strict': True, 'additionalProperties': False}, structured_output_kwargs={})\n",
       "| RunnableBinding(bound=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x00000214002D6F70>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x00000214021BDD30>, root_client=<openai.OpenAI object at 0x000002140010A310>, root_async_client=<openai.AsyncOpenAI object at 0x00000214021BDBE0>, model_name='gpt-5-mini', temperature=1.0, model_kwargs={'extra_headers': {}}, openai_api_key=SecretStr('**********'), top_p=1.0), kwargs={'response_format': {'type': 'json_schema', 'json_schema': {'name': 'answer', 'description': 'Extract the answer', 'strict': True, 'schema': {'type': 'object', 'properties': {'answer': {'type': 'string', 'description': 'The answer from the LLM to the user '}}, 'required': ['answer'], 'strict': True, 'additionalProperties': False}}}, 'ls_structured_output_format': {'kwargs': {'method': 'json_schema', 'strict': None}, 'schema': {'type': 'function', 'function': {'name': 'answer', 'description': 'Extract the answer', 'parameters': {'type': 'object', 'properties': {'answer': {'type': 'string', 'description': 'The answer from the LLM to the user '}}, 'required': ['answer'], 'strict': True, 'additionalProperties': False}}}}}, config={}, config_factories=[])\n",
       "| JsonOutputParser()"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool! Now let's hydrate our prompt by calling .invoke() with our inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': '¡Argh! ¿Capitán yo? Aún no, grumete — sigo buscando un bergantín, una tripulación leal y un mapa hacia el oro. Pero guarda tu ron: cuando tenga cubierta y bandera, mandaré con puño de hierro. ¿Te unes?'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hydrated_prompt = prompt.invoke({\"question\": \"Are you a captain yet?\", \"language\": \"Spanish\"})\n",
    "hydrated_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now let's pass those messages to OpenAI and see what we get back!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content='No soy capitán todavía, pero ¡mi corazón es valiente y mi espada siempre lista! ¡Aventuras en el mar me esperan!', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None)\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "from langsmith import Client\n",
    "from langsmith.client import convert_prompt_to_openai_format\n",
    "\n",
    "client = Client()\n",
    "\n",
    "prompt = client.pull_prompt(\"pirate-friend\", include_model=False)\n",
    "\n",
    "variables = {\n",
    "    \"question\": \"Are you a captain yet?\",\n",
    "    \"language\": \"Spanish\"\n",
    "}\n",
    "\n",
    "\n",
    "hydrated_prompt = prompt.invoke(variables)\n",
    "\n",
    "converted = convert_prompt_to_openai_format(hydrated_prompt)\n",
    "converted_messages = converted[\"messages\"]  \n",
    "\n",
    "openai_client = OpenAI()\n",
    "\n",
    "response = openai_client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=converted_messages,\n",
    ")\n",
    "\n",
    "print(response.choices[0].message)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### [Extra: LangChain Only] Pulling down the Model Configuration\n",
    "\n",
    "We can also pull down the saved model configuration as a LangChain RunnableBinding when we use `include_model=True`. This allows us to run our prompt template directly with the saved model configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructuredPrompt(input_variables=['language', 'question'], input_types={}, partial_variables={}, metadata={'lc_hub_owner': '-', 'lc_hub_repo': 'pirate-friend', 'lc_hub_commit_hash': '826e06f1702c5777d78dc1cb06bd04ae8eb55f23a82b6504752d44a4cb95bd31'}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['language'], input_types={}, partial_variables={}, template='You are a pirate from 1600s, you only seak {language}'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='{question}'), additional_kwargs={})], schema_={'title': 'answer', 'description': 'Extract the answer', 'type': 'object', 'properties': {'answer': {'type': 'string', 'description': 'The answer from the LLM to the user '}}, 'required': ['answer'], 'strict': True, 'additionalProperties': False}, structured_output_kwargs={})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test out your prompt!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='You are a pirate from 1600s, you only seak Spanish', additional_kwargs={}, response_metadata={}), HumanMessage(content='Are you a captain yet?', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.invoke({\"question\": \"Are you a captain yet?\", \"language\": \"Spanish\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pull down a specific commit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pull down a specific commit from the Prompt Hub by pasting in the code snippet from the UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ayush singh\\appdata\\local\\programs\\python\\python39\\lib\\json\\decoder.py:337: UserWarning: WARNING! extra_headers is not default parameter.\n",
      "                extra_headers was transferred to model_kwargs.\n",
      "                Please confirm that extra_headers is what you intended.\n",
      "  obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langsmith import Client\n",
    "LANGCHAIN_API_KEY=os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "client = Client(api_key=LANGCHAIN_API_KEY)\n",
    "prompt = client.pull_prompt(\"pirate-friend:53d032f6\", include_model=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this commit!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content=\"Arrr, matey! The world of 2500 be a far cry from the days of old! The seas be teemin' with advanced ships, powered by clean energy and navigated by AI. Landlubbers be livin' in mega-cities that stretch high into the skies, with smart technology makin' life easier, though some still long for the simplicity of the past.\\n\\nThe oceans, once plundered and polluted, be recoverin' thanks to technology and conservation efforts. Be it sea creatures or treasure, there be mysteries yet to uncover beneath the waves. \\n\\nSociety itself be diverse and interconnected, with cultures blendin' and changin' faster than the tides. But beware—there be still conflicts over resources and territories, some pirates may even be seekin' treasures in the remnants of the old world.\\n\\nSo hoist yer flag, keep a weather eye on the horizon, and enjoy the adventure that awaits! Yarrr!\", refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None)\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "from langsmith import Client\n",
    "from langsmith.client import convert_prompt_to_openai_format\n",
    "\n",
    "client = Client()\n",
    "openai_client = OpenAI()\n",
    "\n",
    "prompt = client.pull_prompt(\"pirate-friend\", include_model=False)\n",
    "\n",
    "variables = {\"question\": \"What is the world like?\", \"language\": \"English\"}\n",
    "\n",
    "hydrated_prompt = prompt.invoke(variables)\n",
    "\n",
    "converted = convert_prompt_to_openai_format(hydrated_prompt)\n",
    "converted_messages = converted.get(\"messages\", [])\n",
    "\n",
    "if not converted_messages:\n",
    "    raise ValueError(\"No messages found after converting hydrated prompt. Check your prompt or variables.\")\n",
    "\n",
    "response = openai_client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=converted_messages,\n",
    ")\n",
    "\n",
    "print(response.choices[0].message)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uploading Prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also easily update your prompts in the hub programmatically.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://smith.langchain.com/prompts/french-rag-prompt/75567b82?organizationId=cf5f72fb-2f6d-5b80-8c1e-bdb916a4631f'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "from langsmith import Client\n",
    "\n",
    "client = Client()\n",
    "\n",
    "coding_prompt = \"\"\"You are an expert coding assistant. \n",
    "Use the following pieces of retrieved context to answer the latest question in the conversation.\n",
    "\n",
    "Your answers should be precise, clear, and provide example code if applicable.\n",
    "\n",
    "Conversation: {conversation}\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "\n",
    "# Create a ChatPromptTemplate from the string\n",
    "coding_prompt_template = ChatPromptTemplate.from_template(coding_prompt)\n",
    "\n",
    "# Push the prompt to LangSmith\n",
    "client.push_prompt(\"coding-rag-prompt\", object=coding_prompt_template)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also push a prompt as a RunnableSequence of a prompt and a model. This is useful for storing the model configuration you want to use with this prompt. The provider must be supported by the LangSmith playground."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://smith.langchain.com/prompts/french-runnable-sequence/f1499fb1?organizationId=cf5f72fb-2f6d-5b80-8c1e-bdb916a4631f'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "from langsmith import Client\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "client=Client()\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "french_prompt = \"\"\"You are an assistant for question-answering tasks. \n",
    "Use the following pieces of retrieved context to answer the latest question in the conversation.\n",
    "\n",
    "Your users can only speak French, make sure you only answer your users with French.\n",
    "\n",
    "Conversation: {conversation}\n",
    "Context: {context} \n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "french_prompt_template = ChatPromptTemplate.from_template(french_prompt)\n",
    "chain = french_prompt_template | model\n",
    "client.push_prompt(\"french-runnable-sequence\", object=chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ayush singh\\appdata\\local\\programs\\python\\python39\\lib\\json\\decoder.py:337: UserWarning: WARNING! extra_headers is not default parameter.\n",
      "                extra_headers was transferred to model_kwargs.\n",
      "                Please confirm that extra_headers is what you intended.\n",
      "  obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langsmith import Client\n",
    "LANGCHAIN_API_KEY=os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "client = Client(api_key=LANGCHAIN_API_KEY)\n",
    "prompt1 = client.pull_prompt(\"chess-llm\", include_model=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['', 'question'], input_types={}, partial_variables={}, metadata={'lc_hub_owner': '-', 'lc_hub_repo': 'chess-llm', 'lc_hub_commit_hash': '5148fd61967798e001b814f8d507d076752157c42dd56066426898b5cf48903e'}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[''], input_types={}, partial_variables={}, template='You are a chess related LLM, you answer questions like a {} elo rating chess player.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='{question}'), additional_kwargs={})])\n",
       "| RunnableBinding(bound=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x0000021412673430>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x0000021412673B50>, root_client=<openai.OpenAI object at 0x0000021412673220>, root_async_client=<openai.AsyncOpenAI object at 0x0000021412673EE0>, model_name='gpt-5-mini', model_kwargs={'extra_headers': {}}, openai_api_key=SecretStr('**********'), top_p=1.0), kwargs={}, config={}, config_factories=[])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content=\"The move 1...Be4 could be a response to an existing position, but without more context about the overall position on the board or the specific question, it's a bit tricky to recommend a precise counter move. However, generally speaking, you might consider a few different types of responses:\\n\\n1. If the bishop on e4 is hanging (meaning it can be captured), then you could simply capture it with your knight or another piece if possible.\\n2. If the e4 bishop is well-supported and you’re not in a position to capture, you might consider moves that challenge its influence, such as playing f3 (if it’s your bishop) or potentially repositioning your pieces to attack other targets.\\n3. If you're looking to create tension in the center, you might consider moves such as d5 if the position allows that.\\n\\nCould you provide a bit more context, like the full position or the color you are playing so I can give more specific advice?\", refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None)\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "from langsmith import Client\n",
    "from langsmith.client import convert_prompt_to_openai_format\n",
    "\n",
    "client = Client()\n",
    "openai_client = OpenAI()\n",
    "\n",
    "prompt = client.pull_prompt(\"chess-llm\", include_model=False)\n",
    "\n",
    "variables = {\n",
    "    \"rating\": \"1400\", \n",
    "    \"question\": \"What should be the counter move for bishop e4?\"  \n",
    "}\n",
    "\n",
    "hydrated_prompt = prompt.invoke(variables)\n",
    "\n",
    "converted_messages = convert_prompt_to_openai_format(hydrated_prompt)[\"messages\"]\n",
    "\n",
    "response = openai_client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=converted_messages\n",
    ")\n",
    "\n",
    "print(response.choices[0].message)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['', 'question'], input_types={}, partial_variables={}, metadata={'lc_hub_owner': '-', 'lc_hub_repo': 'chess-llm', 'lc_hub_commit_hash': '5148fd61967798e001b814f8d507d076752157c42dd56066426898b5cf48903e'}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[''], input_types={}, partial_variables={}, template='You are a chess related LLM, you answer questions like a {} elo rating chess player.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='{question}'), additional_kwargs={})])\n",
       "| RunnableBinding(bound=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x0000021412673430>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x0000021412673B50>, root_client=<openai.OpenAI object at 0x0000021412673220>, root_async_client=<openai.AsyncOpenAI object at 0x0000021412673EE0>, model_name='gpt-5-mini', model_kwargs={'extra_headers': {}}, openai_api_key=SecretStr('**********'), top_p=1.0), kwargs={}, config={}, config_factories=[])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messages=[SystemMessage(content='You are a chess related LLM, you answer questions like a 1400 elo rating chess player.', additional_kwargs={}, response_metadata={}), HumanMessage(content='What should be the counter move for bishop e4?', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "variables = {\n",
    "    \"rating\": \"1400\", \n",
    "    \"question\": \"What should be the counter move for bishop e4?\"  \n",
    "}\n",
    "\n",
    "hydrated_prompt = prompt.invoke(variables)\n",
    "\n",
    "print(hydrated_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ayush singh\\appdata\\local\\programs\\python\\python39\\lib\\json\\decoder.py:337: UserWarning: WARNING! extra_headers is not default parameter.\n",
      "                extra_headers was transferred to model_kwargs.\n",
      "                Please confirm that extra_headers is what you intended.\n",
      "  obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langsmith import Client\n",
    "LANGCHAIN_API_KEY=os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "client = Client(api_key=LANGCHAIN_API_KEY)\n",
    "prompt = client.pull_prompt(\"chess-llm:7820599e\", include_model=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content=\"To better assist you, could you clarify the context of the position? What is your opponent's setup, and what is your piece placement? Understanding the whole board or the moves that led to Bishop e4 will help me guide you effectively.\", refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None)\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "from langsmith import Client\n",
    "from langsmith.client import convert_prompt_to_openai_format\n",
    "\n",
    "client = Client()\n",
    "openai_client = OpenAI()\n",
    "\n",
    "prompt = client.pull_prompt(\"chess-llm\", include_model=False)\n",
    "\n",
    "variables = {\n",
    "    \"question\": \"What should be the counter move for bishop e4?\",\n",
    "    \"rating\": \"1400\" \n",
    "}\n",
    "\n",
    "hydrated_prompt = prompt.invoke(variables)\n",
    "\n",
    "converted = convert_prompt_to_openai_format(hydrated_prompt)\n",
    "converted_messages = converted.get(\"messages\", [])\n",
    "\n",
    "if not converted_messages:\n",
    "    raise ValueError(\"No messages found after converting hydrated prompt. Check your prompt or variables.\")\n",
    "\n",
    "response = openai_client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=converted_messages\n",
    ")\n",
    "\n",
    "print(response.choices[0].message)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://smith.langchain.com/prompts/coding-rag-prompt/407049c3?organizationId=cf5f72fb-2f6d-5b80-8c1e-bdb916a4631f'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "from langsmith import Client\n",
    "\n",
    "client = Client()\n",
    "\n",
    "coding_prompt = \"\"\"You are an expert coding assistant. \n",
    "Use the following pieces of retrieved context to answer the latest question in the conversation.\n",
    "\n",
    "Your answers should be precise, clear, and provide example code if applicable.\n",
    "\n",
    "Conversation: {conversation}\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "\n",
    "coding_prompt_template = ChatPromptTemplate.from_template(coding_prompt)\n",
    "\n",
    "client.push_prompt(\"coding-rag-prompt\", object=coding_prompt_template)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://smith.langchain.com/prompts/coding-runnable-sequence/24a3b079?organizationId=cf5f72fb-2f6d-5b80-8c1e-bdb916a4631f'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "from langsmith import Client\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "client = Client()\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.3)\n",
    "\n",
    "coding_prompt = \"\"\"You are an expert coding assistant. \n",
    "Use the following pieces of retrieved context to answer the latest question in the conversation.\n",
    "\n",
    "Your answers should be precise, clear, and provide example code if applicable.\n",
    "\n",
    "Conversation: {conversation}\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "\n",
    "coding_prompt_template = ChatPromptTemplate.from_template(coding_prompt)\n",
    "\n",
    "chain = coding_prompt_template | model\n",
    "\n",
    "client.push_prompt(\"coding-runnable-sequence\", object=chain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
